{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddcdae62",
   "metadata": {},
   "source": [
    "# Preprocessing data | recovery-news-data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822b090b",
   "metadata": {},
   "source": [
    "*CS 539 - Social Media Mining | Francesca Spezzano*\n",
    "\n",
    "*Computer Science | Boise State University*\n",
    "\n",
    "*11.23.2022 | Fall 2022*\n",
    "\n",
    "*Aida Gomezbueno Berezo | aidagomezbuenobe@u.boisestate.edu*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87f71a4",
   "metadata": {},
   "source": [
    "\n",
    "Launching notebook with the following command:\n",
    "\n",
    "> *jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e100000000000*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbbc543",
   "metadata": {},
   "source": [
    "#### Importing libraries | Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac961c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import nltk\n",
    "import numpy as np\n",
    "import num2words\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import time\n",
    "from datetime import datetime\n",
    "from nltk.stem import *\n",
    "from nltk.corpus import names, stopwords, words\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import sparse\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67570d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>image</th>\n",
       "      <th>body_text</th>\n",
       "      <th>political_bias</th>\n",
       "      <th>country</th>\n",
       "      <th>reliability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.nytimes.com/article/what-is-corona...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>2020-01-21</td>\n",
       "      <td>['Knvul Sheikh', 'Roni Caryn Rabin']</td>\n",
       "      <td>The Coronavirus: What Scientists Have Learned ...</td>\n",
       "      <td>https://static01.nyt.com/images/2020/03/12/sci...</td>\n",
       "      <td>\\nA novel respiratory virus that originated in...</td>\n",
       "      <td>Left</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.npr.org/2020/01/22/798392172/chine...</td>\n",
       "      <td>National Public Radio (NPR)</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>['Emily Feng']</td>\n",
       "      <td>Chinese Health Officials: More Die From Newly ...</td>\n",
       "      <td>https://media.npr.org/include/images/facebook-...</td>\n",
       "      <td>Chinese Health Officials: More Die From Newly ...</td>\n",
       "      <td>Center</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://www.theverge.com/2020/1/23/21078457/co...</td>\n",
       "      <td>The Verge</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>['Nicole Wetsman']</td>\n",
       "      <td>Everything you need to know about the coronavirus</td>\n",
       "      <td>https://cdn.vox-cdn.com/thumbor/a9_Oz7cvSBKyal...</td>\n",
       "      <td>Public health experts around the globe are scr...</td>\n",
       "      <td>Left-center</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://www.worldhealth.net/news/novel-coronav...</td>\n",
       "      <td>WorldHealth.Net</td>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>[]</td>\n",
       "      <td>Novel Coronavirus Cases Confirmed To Be Spreading</td>\n",
       "      <td>https://www.worldhealth.net/media/original_ima...</td>\n",
       "      <td>The first two coronavirus cases in Europe have...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://www.theverge.com/2020/1/24/21080845/co...</td>\n",
       "      <td>The Verge</td>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>['Nicole Wetsman', 'Zoe Schiffer', 'Jay Peters...</td>\n",
       "      <td>Coronavirus disrupts the world: updates on the...</td>\n",
       "      <td>https://cdn.vox-cdn.com/thumbor/t2gt1SmEni4Mcr...</td>\n",
       "      <td>A new coronavirus appeared in Wuhan, China, at...</td>\n",
       "      <td>Left-center</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   news_id                                                url  \\\n",
       "0        0  https://www.nytimes.com/article/what-is-corona...   \n",
       "1        1  https://www.npr.org/2020/01/22/798392172/chine...   \n",
       "2        2  https://www.theverge.com/2020/1/23/21078457/co...   \n",
       "3        3  https://www.worldhealth.net/news/novel-coronav...   \n",
       "4        4  https://www.theverge.com/2020/1/24/21080845/co...   \n",
       "\n",
       "                     publisher publish_date  \\\n",
       "0           The New York Times   2020-01-21   \n",
       "1  National Public Radio (NPR)   2020-01-22   \n",
       "2                    The Verge   2020-01-23   \n",
       "3              WorldHealth.Net   2020-01-24   \n",
       "4                    The Verge   2020-01-24   \n",
       "\n",
       "                                              author  \\\n",
       "0               ['Knvul Sheikh', 'Roni Caryn Rabin']   \n",
       "1                                     ['Emily Feng']   \n",
       "2                                 ['Nicole Wetsman']   \n",
       "3                                                 []   \n",
       "4  ['Nicole Wetsman', 'Zoe Schiffer', 'Jay Peters...   \n",
       "\n",
       "                                               title  \\\n",
       "0  The Coronavirus: What Scientists Have Learned ...   \n",
       "1  Chinese Health Officials: More Die From Newly ...   \n",
       "2  Everything you need to know about the coronavirus   \n",
       "3  Novel Coronavirus Cases Confirmed To Be Spreading   \n",
       "4  Coronavirus disrupts the world: updates on the...   \n",
       "\n",
       "                                               image  \\\n",
       "0  https://static01.nyt.com/images/2020/03/12/sci...   \n",
       "1  https://media.npr.org/include/images/facebook-...   \n",
       "2  https://cdn.vox-cdn.com/thumbor/a9_Oz7cvSBKyal...   \n",
       "3  https://www.worldhealth.net/media/original_ima...   \n",
       "4  https://cdn.vox-cdn.com/thumbor/t2gt1SmEni4Mcr...   \n",
       "\n",
       "                                           body_text political_bias country  \\\n",
       "0  \\nA novel respiratory virus that originated in...           Left     USA   \n",
       "1  Chinese Health Officials: More Die From Newly ...         Center     USA   \n",
       "2  Public health experts around the globe are scr...    Left-center     USA   \n",
       "3  The first two coronavirus cases in Europe have...            NaN     USA   \n",
       "4  A new coronavirus appeared in Wuhan, China, at...    Left-center     USA   \n",
       "\n",
       "   reliability  \n",
       "0            1  \n",
       "1            1  \n",
       "2            1  \n",
       "3            0  \n",
       "4            1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat = pd.read_csv(r'recovery-news-data.csv')\n",
    "df = pd.DataFrame(dat)\n",
    "df.columns = ['index', 'news_id', 'url', 'publisher', 'publish_date', 'author', 'title', 'image', 'body_text', 'political_bias', 'country', 'reliability']\n",
    "df = df.drop('index', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c20781",
   "metadata": {},
   "source": [
    "#### Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76d89a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_subs = ['https://', 'http://', 'www.']\n",
    "substring = \"/\"\n",
    "urls=[]\n",
    "for i in df['url']:\n",
    "    for j in clean_subs:\n",
    "        if j in i:\n",
    "            i = i.split(j)\n",
    "            i = i[1]\n",
    "    if substring in i:\n",
    "        i = i.split(substring)\n",
    "        i = i[0]\n",
    "    urls.append(i)\n",
    "df['url']=urls\n",
    "\n",
    "df['publish_date'] = df['publish_date'].fillna(\"1970-11-01\")\n",
    "epoch = datetime.utcfromtimestamp(0)\n",
    "for i in df['publish_date']:\n",
    "    date_object = datetime.strptime(str(i), '%Y-%m-%d').date()\n",
    "    delta = date_object - epoch.date()\n",
    "    dt = datetime.fromtimestamp(int(delta.total_seconds())).strftime('%Y-%m-%d')\n",
    "    mth = datetime.fromtimestamp(int(delta.total_seconds())).strftime('%B')\n",
    "    if mth==\"October\":\n",
    "        mth = \"UNKNOWN\"\n",
    "    df['publish_date'] = df['publish_date'].replace(i, mth)\n",
    "df['author'] = df['author'].fillna(\"\")\n",
    "replace_simb = ['[', ']', \"'\", ' etc.', \"‘\"]\n",
    "coma = \", \"\n",
    "array_col = []\n",
    "for i in df['author']:\n",
    "    temp = []\n",
    "    for j in replace_simb:\n",
    "        if j in i:\n",
    "            i = i.replace(j, \"\")\n",
    "            \n",
    "df['political_bias'] = df['political_bias'].fillna(\"NEUTRAL\")\n",
    "df['image'] = df['image'].fillna(\"\")\n",
    "df['title'] = df['title'].fillna(\"\")\n",
    "df['country'] = df['country'].fillna(\"UNKNOWN\")\n",
    "\n",
    "df['alltext'] = df['publisher'] + \" \" + df['author'] + \" \" +df['political_bias'] + \" \" + df['title'] + \" \" + df['body_text'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b06aad",
   "metadata": {},
   "source": [
    "#### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd3c17a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>image</th>\n",
       "      <th>body_text</th>\n",
       "      <th>political_bias</th>\n",
       "      <th>country</th>\n",
       "      <th>reliability</th>\n",
       "      <th>alltext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>January</td>\n",
       "      <td>['Knvul Sheikh', 'Roni Caryn Rabin']</td>\n",
       "      <td>The Coronavirus: What Scientists Have Learned ...</td>\n",
       "      <td>https://static01.nyt.com/images/2020/03/12/sci...</td>\n",
       "      <td>\\nA novel respiratory virus that originated in...</td>\n",
       "      <td>Left</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>The New York Times ['Knvul Sheikh', 'Roni Cary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>npr.org</td>\n",
       "      <td>National Public Radio (NPR)</td>\n",
       "      <td>January</td>\n",
       "      <td>['Emily Feng']</td>\n",
       "      <td>Chinese Health Officials: More Die From Newly ...</td>\n",
       "      <td>https://media.npr.org/include/images/facebook-...</td>\n",
       "      <td>Chinese Health Officials: More Die From Newly ...</td>\n",
       "      <td>Center</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>National Public Radio (NPR) ['Emily Feng'] Cen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>theverge.com</td>\n",
       "      <td>The Verge</td>\n",
       "      <td>January</td>\n",
       "      <td>['Nicole Wetsman']</td>\n",
       "      <td>Everything you need to know about the coronavirus</td>\n",
       "      <td>https://cdn.vox-cdn.com/thumbor/a9_Oz7cvSBKyal...</td>\n",
       "      <td>Public health experts around the globe are scr...</td>\n",
       "      <td>Left-center</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>The Verge ['Nicole Wetsman'] Left-center Every...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>worldhealth.net</td>\n",
       "      <td>WorldHealth.Net</td>\n",
       "      <td>January</td>\n",
       "      <td>[]</td>\n",
       "      <td>Novel Coronavirus Cases Confirmed To Be Spreading</td>\n",
       "      <td>https://www.worldhealth.net/media/original_ima...</td>\n",
       "      <td>The first two coronavirus cases in Europe have...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>USA</td>\n",
       "      <td>0</td>\n",
       "      <td>WorldHealth.Net [] NEUTRAL Novel Coronavirus C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>theverge.com</td>\n",
       "      <td>The Verge</td>\n",
       "      <td>January</td>\n",
       "      <td>['Nicole Wetsman', 'Zoe Schiffer', 'Jay Peters...</td>\n",
       "      <td>Coronavirus disrupts the world: updates on the...</td>\n",
       "      <td>https://cdn.vox-cdn.com/thumbor/t2gt1SmEni4Mcr...</td>\n",
       "      <td>A new coronavirus appeared in Wuhan, China, at...</td>\n",
       "      <td>Left-center</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>The Verge ['Nicole Wetsman', 'Zoe Schiffer', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   news_id              url                    publisher publish_date  \\\n",
       "0        0      nytimes.com           The New York Times      January   \n",
       "1        1          npr.org  National Public Radio (NPR)      January   \n",
       "2        2     theverge.com                    The Verge      January   \n",
       "3        3  worldhealth.net              WorldHealth.Net      January   \n",
       "4        4     theverge.com                    The Verge      January   \n",
       "\n",
       "                                              author  \\\n",
       "0               ['Knvul Sheikh', 'Roni Caryn Rabin']   \n",
       "1                                     ['Emily Feng']   \n",
       "2                                 ['Nicole Wetsman']   \n",
       "3                                                 []   \n",
       "4  ['Nicole Wetsman', 'Zoe Schiffer', 'Jay Peters...   \n",
       "\n",
       "                                               title  \\\n",
       "0  The Coronavirus: What Scientists Have Learned ...   \n",
       "1  Chinese Health Officials: More Die From Newly ...   \n",
       "2  Everything you need to know about the coronavirus   \n",
       "3  Novel Coronavirus Cases Confirmed To Be Spreading   \n",
       "4  Coronavirus disrupts the world: updates on the...   \n",
       "\n",
       "                                               image  \\\n",
       "0  https://static01.nyt.com/images/2020/03/12/sci...   \n",
       "1  https://media.npr.org/include/images/facebook-...   \n",
       "2  https://cdn.vox-cdn.com/thumbor/a9_Oz7cvSBKyal...   \n",
       "3  https://www.worldhealth.net/media/original_ima...   \n",
       "4  https://cdn.vox-cdn.com/thumbor/t2gt1SmEni4Mcr...   \n",
       "\n",
       "                                           body_text political_bias country  \\\n",
       "0  \\nA novel respiratory virus that originated in...           Left     USA   \n",
       "1  Chinese Health Officials: More Die From Newly ...         Center     USA   \n",
       "2  Public health experts around the globe are scr...    Left-center     USA   \n",
       "3  The first two coronavirus cases in Europe have...        NEUTRAL     USA   \n",
       "4  A new coronavirus appeared in Wuhan, China, at...    Left-center     USA   \n",
       "\n",
       "   reliability                                            alltext  \n",
       "0            1  The New York Times ['Knvul Sheikh', 'Roni Cary...  \n",
       "1            1  National Public Radio (NPR) ['Emily Feng'] Cen...  \n",
       "2            1  The Verge ['Nicole Wetsman'] Left-center Every...  \n",
       "3            0  WorldHealth.Net [] NEUTRAL Novel Coronavirus C...  \n",
       "4            1  The Verge ['Nicole Wetsman', 'Zoe Schiffer', '...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a6e987",
   "metadata": {},
   "source": [
    "#### Preprocessing | Basic Steps\n",
    "\n",
    "    1. Lowercase. \n",
    "    2. Stop words.\n",
    "    3. Punctuation/Symbols.\n",
    "    4. Apostophre.\n",
    "    5. Single characters.\n",
    "    6. Stemming.\n",
    "    7. Lemmatisation.\n",
    "    8. Converting Numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c5ce545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toLowerCase(df):\n",
    "    df['alltext'] = df['alltext'].str.lower()\n",
    "    return df\n",
    "\n",
    "def delStopWords(df):\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    valid_allt=[]\n",
    "    for i in df['alltext']:\n",
    "        valid_t=[]\n",
    "        for j in i:\n",
    "            if j not in stop_words:\n",
    "                valid_t.append(j)\n",
    "        valid_allt.append(valid_t)\n",
    "    df['alltext']=valid_allt\n",
    "    return df\n",
    "\n",
    "def toListOfLists(df):\n",
    "    all_text = []\n",
    "    for i in df['alltext']:\n",
    "        i = i.split()\n",
    "        all_text.append(i)\n",
    "    df['alltext']=all_text\n",
    "    return df  \n",
    "\n",
    "def delSymbols(df):\n",
    "    symbols = \"!\\\"“‘’”#$%&()*+-./:;<,=>?@[\\]^_`{|}~\\n►●…\"\n",
    "    valid_allt=[]\n",
    "    for i in df['alltext']:\n",
    "        valid_t = []\n",
    "        for j in i:\n",
    "            for k in symbols:\n",
    "                if k in j:\n",
    "                    j = j.replace(k, \"\")\n",
    "            valid_t.append(j)\n",
    "        if \"[]\" in valid_t:\n",
    "            valid_t.remove(\"[]\")\n",
    "        valid_allt.append(valid_t)\n",
    "    df['alltext']=valid_allt\n",
    "    return df\n",
    "\n",
    "def delApostrophe(df):\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    valid_alltext=[]\n",
    "    for i in df['alltext']:\n",
    "        valid_text=[]\n",
    "        for j in i:\n",
    "            if j not in stop_words:\n",
    "                if \"'\" in j:\n",
    "                    j=j.replace(\"'\", \"\")\n",
    "                valid_text.append(j)\n",
    "        valid_alltext.append(valid_text)\n",
    "    df['alltext']=valid_alltext\n",
    "    return df\n",
    "\n",
    "def delSingleChars(df):\n",
    "    valid_allt=[]\n",
    "    for i in df['alltext']:\n",
    "        valid_t = []\n",
    "        for j in i:\n",
    "            if len(j)>1:\n",
    "                valid_t.append(j)\n",
    "        valid_allt.append(valid_t)\n",
    "    df['alltext']=valid_allt\n",
    "    return df\n",
    "\n",
    "#NOMBRES PROPIOS CON PUNTOS.\n",
    "def remainDomains(df):\n",
    "    domain_dot=[]\n",
    "    for i in df['publisher']:\n",
    "        if \".\" in i:\n",
    "            domain_dot.append(str(i).lower())\n",
    "    domain_dot = np.unique(domain_dot)\n",
    "    valid_allt=[]\n",
    "    for i in df['alltext']:\n",
    "        valid_t=[]\n",
    "        for j in i:\n",
    "            for k in domain_dot:\n",
    "                if \".\" in k:\n",
    "                    k_wodot = k.replace(\".\", \"\")\n",
    "                    if k_wodot in j:\n",
    "                        j = j.replace(j, k)\n",
    "            valid_t.append(j)\n",
    "        valid_allt.append(valid_t)\n",
    "    df['alltext']=valid_allt    \n",
    "    return df\n",
    "\n",
    "#THE ORDER IS LEMMATIZATION THEN STEMMING, OR JUST STEMMING.\n",
    "def lemmatize(df):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    valid_allt=[]\n",
    "    for i in df['alltext']:\n",
    "        valid_t=[]\n",
    "        for j in i:\n",
    "            j = wnl.lemmatize(j)\n",
    "            valid_t.append(j)\n",
    "        valid_allt.append(valid_t) \n",
    "    df['alltext']=valid_allt\n",
    "    return df\n",
    "\n",
    "#STEMMING\n",
    "def stem(df):\n",
    "    stemmer = PorterStemmer()\n",
    "    valid_allt=[]\n",
    "    for i in df['alltext']:\n",
    "        valid_t=[]\n",
    "        for j in i:\n",
    "            j = stemmer.stem(j)\n",
    "            valid_t.append(j)\n",
    "        valid_allt.append(valid_t) \n",
    "    df['alltext']=valid_allt\n",
    "    return df\n",
    "    #df.to_csv('preprocessing-numbers.csv', index=False)\n",
    "    \n",
    "#converting numbers\n",
    "def num_conversion(j):\n",
    "    k, l = j.split()\n",
    "    k = float(k)\n",
    "    l = float(l)\n",
    "    j = str(k*l)\n",
    "    return j\n",
    "\n",
    "def convertNumbers(df):\n",
    "    valid_allt=[]\n",
    "    for i in df['alltext']:\n",
    "        valid_t = []\n",
    "        for j in i:\n",
    "            if j.isnumeric():\n",
    "                if \"½\" in j:\n",
    "                    j = j.replace(\"½\", \" 0.5\")\n",
    "                    j = num_conversion(j)\n",
    "                if \"¼\" in j:\n",
    "                    j = j.replace(\"¼\", \" 0.25\")\n",
    "                    j = num_conversion(j)\n",
    "                if \"⅔\" in j:\n",
    "                    j = j.replace(\"⅔\", \" 0.67\")\n",
    "                    j = num_conversion(j)\n",
    "                if \"¾\" in j:\n",
    "                    j = j.replace(\"¾\", \" 0.75\")\n",
    "                    j = num_conversion(j)\n",
    "                if \"⅓\" in j:\n",
    "                    j = j.replace(\"⅓\", \" 0.33\")\n",
    "                    j = num_conversion(j)\n",
    "                j = num2words.num2words(float(j))\n",
    "            valid_t.append(j)          \n",
    "        valid_allt.append(valid_t)\n",
    "    df['alltext']=valid_allt\n",
    "    return df\n",
    "    #df.to_csv('preprocessing-no_numbers.csv', index=False)\n",
    "\n",
    "def preprocess(df):\n",
    "    df = toLowerCase(df)\n",
    "    df = toListOfLists(df)\n",
    "    df = delStopWords(df)\n",
    "    df = delSymbols(df)\n",
    "    df = delStopWords(df)\n",
    "    df = delApostrophe(df)\n",
    "    df = delSingleChars(df)\n",
    "    df = lemmatize(df)\n",
    "    df = stem(df)\n",
    "    df = convertNumbers(df)\n",
    "    df = delSymbols(df)\n",
    "    df = remainDomains(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80ebaf1",
   "metadata": {},
   "source": [
    "#### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d11c166d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>image</th>\n",
       "      <th>body_text</th>\n",
       "      <th>political_bias</th>\n",
       "      <th>country</th>\n",
       "      <th>reliability</th>\n",
       "      <th>alltext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>January</td>\n",
       "      <td>['Knvul Sheikh', 'Roni Caryn Rabin']</td>\n",
       "      <td>The Coronavirus: What Scientists Have Learned ...</td>\n",
       "      <td>https://static01.nyt.com/images/2020/03/12/sci...</td>\n",
       "      <td>\\nA novel respiratory virus that originated in...</td>\n",
       "      <td>Left</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>[new, york, time, knvul, sheikh, roni, caryn, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>npr.org</td>\n",
       "      <td>National Public Radio (NPR)</td>\n",
       "      <td>January</td>\n",
       "      <td>['Emily Feng']</td>\n",
       "      <td>Chinese Health Officials: More Die From Newly ...</td>\n",
       "      <td>https://media.npr.org/include/images/facebook-...</td>\n",
       "      <td>Chinese Health Officials: More Die From Newly ...</td>\n",
       "      <td>Center</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>[nation, public, radio, npr, emili, feng, cent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>theverge.com</td>\n",
       "      <td>The Verge</td>\n",
       "      <td>January</td>\n",
       "      <td>['Nicole Wetsman']</td>\n",
       "      <td>Everything you need to know about the coronavirus</td>\n",
       "      <td>https://cdn.vox-cdn.com/thumbor/a9_Oz7cvSBKyal...</td>\n",
       "      <td>Public health experts around the globe are scr...</td>\n",
       "      <td>Left-center</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>[verg, nicol, wetsman, leftcent, everyth, need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>worldhealth.net</td>\n",
       "      <td>WorldHealth.Net</td>\n",
       "      <td>January</td>\n",
       "      <td>[]</td>\n",
       "      <td>Novel Coronavirus Cases Confirmed To Be Spreading</td>\n",
       "      <td>https://www.worldhealth.net/media/original_ima...</td>\n",
       "      <td>The first two coronavirus cases in Europe have...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>USA</td>\n",
       "      <td>0</td>\n",
       "      <td>[worldhealth.net, neutral, novel, coronaviru, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>theverge.com</td>\n",
       "      <td>The Verge</td>\n",
       "      <td>January</td>\n",
       "      <td>['Nicole Wetsman', 'Zoe Schiffer', 'Jay Peters...</td>\n",
       "      <td>Coronavirus disrupts the world: updates on the...</td>\n",
       "      <td>https://cdn.vox-cdn.com/thumbor/t2gt1SmEni4Mcr...</td>\n",
       "      <td>A new coronavirus appeared in Wuhan, China, at...</td>\n",
       "      <td>Left-center</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>[verg, nicol, wetsman, zoe, schiffer, jay, pet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   news_id              url                    publisher publish_date  \\\n",
       "0        0      nytimes.com           The New York Times      January   \n",
       "1        1          npr.org  National Public Radio (NPR)      January   \n",
       "2        2     theverge.com                    The Verge      January   \n",
       "3        3  worldhealth.net              WorldHealth.Net      January   \n",
       "4        4     theverge.com                    The Verge      January   \n",
       "\n",
       "                                              author  \\\n",
       "0               ['Knvul Sheikh', 'Roni Caryn Rabin']   \n",
       "1                                     ['Emily Feng']   \n",
       "2                                 ['Nicole Wetsman']   \n",
       "3                                                 []   \n",
       "4  ['Nicole Wetsman', 'Zoe Schiffer', 'Jay Peters...   \n",
       "\n",
       "                                               title  \\\n",
       "0  The Coronavirus: What Scientists Have Learned ...   \n",
       "1  Chinese Health Officials: More Die From Newly ...   \n",
       "2  Everything you need to know about the coronavirus   \n",
       "3  Novel Coronavirus Cases Confirmed To Be Spreading   \n",
       "4  Coronavirus disrupts the world: updates on the...   \n",
       "\n",
       "                                               image  \\\n",
       "0  https://static01.nyt.com/images/2020/03/12/sci...   \n",
       "1  https://media.npr.org/include/images/facebook-...   \n",
       "2  https://cdn.vox-cdn.com/thumbor/a9_Oz7cvSBKyal...   \n",
       "3  https://www.worldhealth.net/media/original_ima...   \n",
       "4  https://cdn.vox-cdn.com/thumbor/t2gt1SmEni4Mcr...   \n",
       "\n",
       "                                           body_text political_bias country  \\\n",
       "0  \\nA novel respiratory virus that originated in...           Left     USA   \n",
       "1  Chinese Health Officials: More Die From Newly ...         Center     USA   \n",
       "2  Public health experts around the globe are scr...    Left-center     USA   \n",
       "3  The first two coronavirus cases in Europe have...        NEUTRAL     USA   \n",
       "4  A new coronavirus appeared in Wuhan, China, at...    Left-center     USA   \n",
       "\n",
       "   reliability                                            alltext  \n",
       "0            1  [new, york, time, knvul, sheikh, roni, caryn, ...  \n",
       "1            1  [nation, public, radio, npr, emili, feng, cent...  \n",
       "2            1  [verg, nicol, wetsman, leftcent, everyth, need...  \n",
       "3            0  [worldhealth.net, neutral, novel, coronaviru, ...  \n",
       "4            1  [verg, nicol, wetsman, zoe, schiffer, jay, pet...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bb3528",
   "metadata": {},
   "source": [
    "#### TF-IDF Score\n",
    "> #### TF (Term Frequency). TF is individual to each document and word. Frequency of a term in relation to the doc that belongs to.\n",
    "> * TF(t,d) = count of t in d.\n",
    "\n",
    "> #### DF (Document Frequency). Number of documents in which the word is present.\n",
    "> * DF(t) = occurrence of t in N documents.\n",
    "\n",
    "> #### IDF (Inverse Document Frequency). Informativeness of term t.\n",
    "> * IDF(t) = log(D/df).\n",
    "\n",
    "> #### TF-IDF (Term Frequency - Inverse Document Frequency).\n",
    "> * TF-IDF(t,d) = tf(t,d)*log(N/df)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f8a7bb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTF(df):\n",
    "    tf_all = []\n",
    "    n_docs = len(df['alltext'])\n",
    "    n_words_per_doc = {}\n",
    "    w = 0\n",
    "    for i in df['alltext']:\n",
    "        term_freq = {} \n",
    "        uniq_vals = pd.unique(i) \n",
    "        n_words = len(i) \n",
    "        n_words_per_doc[w] = n_words \n",
    "        w+=1\n",
    "        n_uniq_vals = len(uniq_vals)\n",
    "        for x in range(n_uniq_vals): \n",
    "            n=0\n",
    "            temp = uniq_vals[x]\n",
    "            for j in i:\n",
    "                if j==temp: \n",
    "                    n+=1\n",
    "            term_freq[temp]=n \n",
    "        tf_all.append(term_freq) \n",
    "    return df, n_docs, tf_all\n",
    "    \n",
    "def getDF(df, n_docs, tf_all):    \n",
    "    words_ = []\n",
    "    n=0\n",
    "    for x in range(n_docs): \n",
    "        dict_ = tf_all[x] \n",
    "        for y in dict_.keys(): \n",
    "            words_.append(y)\n",
    "            n+=1\n",
    "    words = np.unique(words_) \n",
    "    nw = len(words) \n",
    "    n_news = len(tf_all)\n",
    "    doc_freq={}\n",
    "    for x in range(nw):\n",
    "        n=0\n",
    "        temp = words[x]\n",
    "        for y in range(n_news):\n",
    "            dict_ = tf_all[y]\n",
    "            vals = dict_.keys()\n",
    "            if temp in vals:\n",
    "                n+=1 \n",
    "                continue\n",
    "        doc_freq[temp] = n\n",
    "    return df, doc_freq, nw, words\n",
    "\n",
    "def getIDF(df, doc_freq):\n",
    "    idf={}\n",
    "    n_docs = len(df['alltext'])\n",
    "    for x in doc_freq.keys(): \n",
    "        df_=doc_freq.get(x)\n",
    "        idf_=math.log(n_docs/(df_), 2)\n",
    "        idf[x]=idf_\n",
    "    return df, idf\n",
    "\n",
    "def _getTFIDF(df, tf_all, idf):\n",
    "    tf_idf_allt = []\n",
    "    for x in range(len(tf_all)): \n",
    "        tf_idf={}\n",
    "        dict_tf_doc = tf_all[x]\n",
    "        for y in dict_tf_doc.keys():\n",
    "            tf_r = dict_tf_doc.get(y)\n",
    "            idf_r = idf.get(y)\n",
    "            tf_idf_ = tf_r*idf_r\n",
    "            tf_idf[y]=tf_idf_\n",
    "        tf_idf_allt.append(tf_idf) \n",
    "    df['tf-idf']=tf_idf_allt\n",
    "    return df, tf_idf_allt\n",
    "\n",
    "def getTFIDF(nw, words, tf_idf_allt):\n",
    "    dict_words = {}\n",
    "    for x in range(nw):\n",
    "        tfidf_scores = []\n",
    "        temp = words[x]\n",
    "        for y in tf_idf_allt:\n",
    "            if temp in y:\n",
    "                tfidf_scores.append(y.get(temp))\n",
    "            else:\n",
    "                tfidf_scores.append(0)\n",
    "        dict_words[temp] = tfidf_scores\n",
    "    df_tfidf = pd.DataFrame()\n",
    "    for i in dict_words.keys():\n",
    "        df_tfidf[i] = dict_words.get(i) #array\n",
    "    return df, df_tfidf\n",
    "\n",
    "def _TfidfVectorizer(df):\n",
    "    df, n_docs, tf_all = getTF(df)\n",
    "    df, doc_freq, nw, words = getDF(df, n_docs, tf_all)\n",
    "    df, idf = getIDF(df, doc_freq)\n",
    "    df, tf_idf_allt = _getTFIDF(df, tf_all, idf)\n",
    "    df, df_tfidf = getTFIDF(nw, words, tf_idf_allt)\n",
    "    return df_tfidf\n",
    "\n",
    "def _TfidfVectorizer_(df):\n",
    "    df, n_docs, tf_all = getTF(df)\n",
    "    df, doc_freq, nw, words = getDF(df, n_docs, tf_all)\n",
    "    df, idf = getIDF(df, doc_freq)\n",
    "    df, tf_idf_allt = _getTFIDF(df, tf_all, idf)\n",
    "    df, df_tfidf = getTFIDF(nw, words, tf_idf_allt)\n",
    "    return df_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c01da9e",
   "metadata": {},
   "source": [
    "## TfidfVectorizer w/ Sparse Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ed3221",
   "metadata": {},
   "source": [
    "*In order to be able to train our model with the previous TF-IDF we've computed, we should first convert the DataFrame into a **sparse matrix** (as sklearn does with its TfidfVectorizer). After that, we'll be able to train, test, and finally getting an accuracy higher than 90%.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6ef51492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.31%\n"
     ]
    }
   ],
   "source": [
    "df_tfidf_as_sparse_matrix = sparse.csr_matrix(_TfidfVectorizer(df))\n",
    "\n",
    "X = df_tfidf_as_sparse_matrix.toarray()\n",
    "y = df['reliability']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = LogisticRegression(random_state = 0)\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(pred, y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d615a921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.43%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X = []\n",
    "for each in df['alltext']:\n",
    "    X.append(' '.join(each))\n",
    "X = vectorizer.fit_transform(X)\n",
    "X = X.toarray()\n",
    "y = df['reliability']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = LogisticRegression(random_state = 0)\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(pred, y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbde719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
