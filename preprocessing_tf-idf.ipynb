{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddcdae62",
   "metadata": {},
   "source": [
    "# Preprocessing data | recovery-news-data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822b090b",
   "metadata": {},
   "source": [
    "*CS 539 - Social Media Mining | Francesca Spezzano*\n",
    "\n",
    "*Computer Science | Boise State University*\n",
    "\n",
    "*11.22.2022 | Fall 2022*\n",
    "\n",
    "*Aida Gomezbueno Berezo | aidagomezbuenobe@u.boisestate.edu*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87f71a4",
   "metadata": {},
   "source": [
    "Launching notebook with the following command:\n",
    "\n",
    "*jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e100000000000*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac961c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import names, stopwords, words\n",
    "from nltk.stem import *\n",
    "import num2words\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67570d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>image</th>\n",
       "      <th>body_text</th>\n",
       "      <th>political_bias</th>\n",
       "      <th>country</th>\n",
       "      <th>reliability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.nytimes.com/article/what-is-corona...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>2020-01-21</td>\n",
       "      <td>['Knvul Sheikh', 'Roni Caryn Rabin']</td>\n",
       "      <td>The Coronavirus: What Scientists Have Learned ...</td>\n",
       "      <td>https://static01.nyt.com/images/2020/03/12/sci...</td>\n",
       "      <td>\\nA novel respiratory virus that originated in...</td>\n",
       "      <td>Left</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.npr.org/2020/01/22/798392172/chine...</td>\n",
       "      <td>National Public Radio (NPR)</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>['Emily Feng']</td>\n",
       "      <td>Chinese Health Officials: More Die From Newly ...</td>\n",
       "      <td>https://media.npr.org/include/images/facebook-...</td>\n",
       "      <td>Chinese Health Officials: More Die From Newly ...</td>\n",
       "      <td>Center</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://www.theverge.com/2020/1/23/21078457/co...</td>\n",
       "      <td>The Verge</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>['Nicole Wetsman']</td>\n",
       "      <td>Everything you need to know about the coronavirus</td>\n",
       "      <td>https://cdn.vox-cdn.com/thumbor/a9_Oz7cvSBKyal...</td>\n",
       "      <td>Public health experts around the globe are scr...</td>\n",
       "      <td>Left-center</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://www.worldhealth.net/news/novel-coronav...</td>\n",
       "      <td>WorldHealth.Net</td>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>[]</td>\n",
       "      <td>Novel Coronavirus Cases Confirmed To Be Spreading</td>\n",
       "      <td>https://www.worldhealth.net/media/original_ima...</td>\n",
       "      <td>The first two coronavirus cases in Europe have...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://www.theverge.com/2020/1/24/21080845/co...</td>\n",
       "      <td>The Verge</td>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>['Nicole Wetsman', 'Zoe Schiffer', 'Jay Peters...</td>\n",
       "      <td>Coronavirus disrupts the world: updates on the...</td>\n",
       "      <td>https://cdn.vox-cdn.com/thumbor/t2gt1SmEni4Mcr...</td>\n",
       "      <td>A new coronavirus appeared in Wuhan, China, at...</td>\n",
       "      <td>Left-center</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   news_id                                                url  \\\n",
       "0        0  https://www.nytimes.com/article/what-is-corona...   \n",
       "1        1  https://www.npr.org/2020/01/22/798392172/chine...   \n",
       "2        2  https://www.theverge.com/2020/1/23/21078457/co...   \n",
       "3        3  https://www.worldhealth.net/news/novel-coronav...   \n",
       "4        4  https://www.theverge.com/2020/1/24/21080845/co...   \n",
       "\n",
       "                     publisher publish_date  \\\n",
       "0           The New York Times   2020-01-21   \n",
       "1  National Public Radio (NPR)   2020-01-22   \n",
       "2                    The Verge   2020-01-23   \n",
       "3              WorldHealth.Net   2020-01-24   \n",
       "4                    The Verge   2020-01-24   \n",
       "\n",
       "                                              author  \\\n",
       "0               ['Knvul Sheikh', 'Roni Caryn Rabin']   \n",
       "1                                     ['Emily Feng']   \n",
       "2                                 ['Nicole Wetsman']   \n",
       "3                                                 []   \n",
       "4  ['Nicole Wetsman', 'Zoe Schiffer', 'Jay Peters...   \n",
       "\n",
       "                                               title  \\\n",
       "0  The Coronavirus: What Scientists Have Learned ...   \n",
       "1  Chinese Health Officials: More Die From Newly ...   \n",
       "2  Everything you need to know about the coronavirus   \n",
       "3  Novel Coronavirus Cases Confirmed To Be Spreading   \n",
       "4  Coronavirus disrupts the world: updates on the...   \n",
       "\n",
       "                                               image  \\\n",
       "0  https://static01.nyt.com/images/2020/03/12/sci...   \n",
       "1  https://media.npr.org/include/images/facebook-...   \n",
       "2  https://cdn.vox-cdn.com/thumbor/a9_Oz7cvSBKyal...   \n",
       "3  https://www.worldhealth.net/media/original_ima...   \n",
       "4  https://cdn.vox-cdn.com/thumbor/t2gt1SmEni4Mcr...   \n",
       "\n",
       "                                           body_text political_bias country  \\\n",
       "0  \\nA novel respiratory virus that originated in...           Left     USA   \n",
       "1  Chinese Health Officials: More Die From Newly ...         Center     USA   \n",
       "2  Public health experts around the globe are scr...    Left-center     USA   \n",
       "3  The first two coronavirus cases in Europe have...            NaN     USA   \n",
       "4  A new coronavirus appeared in Wuhan, China, at...    Left-center     USA   \n",
       "\n",
       "   reliability  \n",
       "0            1  \n",
       "1            1  \n",
       "2            1  \n",
       "3            0  \n",
       "4            1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat = pd.read_csv(r'recovery-news-data.csv')\n",
    "df = pd.DataFrame(dat)\n",
    "df.columns = ['index', 'news_id', 'url', 'publisher', 'publish_date', 'author', 'title', 'image', 'body_text', 'political_bias', 'country', 'reliability']\n",
    "df = df.drop('index', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76d89a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>image</th>\n",
       "      <th>body_text</th>\n",
       "      <th>political_bias</th>\n",
       "      <th>country</th>\n",
       "      <th>reliability</th>\n",
       "      <th>alltext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>January</td>\n",
       "      <td>['Knvul Sheikh', 'Roni Caryn Rabin']</td>\n",
       "      <td>The Coronavirus: What Scientists Have Learned ...</td>\n",
       "      <td>https://static01.nyt.com/images/2020/03/12/sci...</td>\n",
       "      <td>\\nA novel respiratory virus that originated in...</td>\n",
       "      <td>Left</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>The New York Times ['Knvul Sheikh', 'Roni Cary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>npr.org</td>\n",
       "      <td>National Public Radio (NPR)</td>\n",
       "      <td>January</td>\n",
       "      <td>['Emily Feng']</td>\n",
       "      <td>Chinese Health Officials: More Die From Newly ...</td>\n",
       "      <td>https://media.npr.org/include/images/facebook-...</td>\n",
       "      <td>Chinese Health Officials: More Die From Newly ...</td>\n",
       "      <td>Center</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>National Public Radio (NPR) ['Emily Feng'] Cen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>theverge.com</td>\n",
       "      <td>The Verge</td>\n",
       "      <td>January</td>\n",
       "      <td>['Nicole Wetsman']</td>\n",
       "      <td>Everything you need to know about the coronavirus</td>\n",
       "      <td>https://cdn.vox-cdn.com/thumbor/a9_Oz7cvSBKyal...</td>\n",
       "      <td>Public health experts around the globe are scr...</td>\n",
       "      <td>Left-center</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>The Verge ['Nicole Wetsman'] Left-center Every...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>worldhealth.net</td>\n",
       "      <td>WorldHealth.Net</td>\n",
       "      <td>January</td>\n",
       "      <td>[]</td>\n",
       "      <td>Novel Coronavirus Cases Confirmed To Be Spreading</td>\n",
       "      <td>https://www.worldhealth.net/media/original_ima...</td>\n",
       "      <td>The first two coronavirus cases in Europe have...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>USA</td>\n",
       "      <td>0</td>\n",
       "      <td>WorldHealth.Net [] NEUTRAL Novel Coronavirus C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>theverge.com</td>\n",
       "      <td>The Verge</td>\n",
       "      <td>January</td>\n",
       "      <td>['Nicole Wetsman', 'Zoe Schiffer', 'Jay Peters...</td>\n",
       "      <td>Coronavirus disrupts the world: updates on the...</td>\n",
       "      <td>https://cdn.vox-cdn.com/thumbor/t2gt1SmEni4Mcr...</td>\n",
       "      <td>A new coronavirus appeared in Wuhan, China, at...</td>\n",
       "      <td>Left-center</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>The Verge ['Nicole Wetsman', 'Zoe Schiffer', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   news_id              url                    publisher publish_date  \\\n",
       "0        0      nytimes.com           The New York Times      January   \n",
       "1        1          npr.org  National Public Radio (NPR)      January   \n",
       "2        2     theverge.com                    The Verge      January   \n",
       "3        3  worldhealth.net              WorldHealth.Net      January   \n",
       "4        4     theverge.com                    The Verge      January   \n",
       "\n",
       "                                              author  \\\n",
       "0               ['Knvul Sheikh', 'Roni Caryn Rabin']   \n",
       "1                                     ['Emily Feng']   \n",
       "2                                 ['Nicole Wetsman']   \n",
       "3                                                 []   \n",
       "4  ['Nicole Wetsman', 'Zoe Schiffer', 'Jay Peters...   \n",
       "\n",
       "                                               title  \\\n",
       "0  The Coronavirus: What Scientists Have Learned ...   \n",
       "1  Chinese Health Officials: More Die From Newly ...   \n",
       "2  Everything you need to know about the coronavirus   \n",
       "3  Novel Coronavirus Cases Confirmed To Be Spreading   \n",
       "4  Coronavirus disrupts the world: updates on the...   \n",
       "\n",
       "                                               image  \\\n",
       "0  https://static01.nyt.com/images/2020/03/12/sci...   \n",
       "1  https://media.npr.org/include/images/facebook-...   \n",
       "2  https://cdn.vox-cdn.com/thumbor/a9_Oz7cvSBKyal...   \n",
       "3  https://www.worldhealth.net/media/original_ima...   \n",
       "4  https://cdn.vox-cdn.com/thumbor/t2gt1SmEni4Mcr...   \n",
       "\n",
       "                                           body_text political_bias country  \\\n",
       "0  \\nA novel respiratory virus that originated in...           Left     USA   \n",
       "1  Chinese Health Officials: More Die From Newly ...         Center     USA   \n",
       "2  Public health experts around the globe are scr...    Left-center     USA   \n",
       "3  The first two coronavirus cases in Europe have...        NEUTRAL     USA   \n",
       "4  A new coronavirus appeared in Wuhan, China, at...    Left-center     USA   \n",
       "\n",
       "   reliability                                            alltext  \n",
       "0            1  The New York Times ['Knvul Sheikh', 'Roni Cary...  \n",
       "1            1  National Public Radio (NPR) ['Emily Feng'] Cen...  \n",
       "2            1  The Verge ['Nicole Wetsman'] Left-center Every...  \n",
       "3            0  WorldHealth.Net [] NEUTRAL Novel Coronavirus C...  \n",
       "4            1  The Verge ['Nicole Wetsman', 'Zoe Schiffer', '...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CLEANING URLS\n",
    "clean_subs = ['https://', 'http://', 'www.']\n",
    "substring = \"/\"\n",
    "urls=[]\n",
    "for i in df['url']:\n",
    "    for j in clean_subs:\n",
    "        if j in i:\n",
    "            i = i.split(j)\n",
    "            i = i[1]\n",
    "    if substring in i:\n",
    "        i = i.split(substring)\n",
    "        i = i[0]\n",
    "    urls.append(i)\n",
    "df['url']=urls\n",
    "\n",
    "df['publish_date'] = df['publish_date'].fillna(\"1970-11-01\")\n",
    "epoch = datetime.utcfromtimestamp(0)\n",
    "for i in df['publish_date']:\n",
    "    date_object = datetime.strptime(str(i), '%Y-%m-%d').date()\n",
    "    delta = date_object - epoch.date()\n",
    "    dt = datetime.fromtimestamp(int(delta.total_seconds())).strftime('%Y-%m-%d')\n",
    "    mth = datetime.fromtimestamp(int(delta.total_seconds())).strftime('%B')\n",
    "    if mth==\"October\":\n",
    "        mth = \"UNKNOWN\"\n",
    "    df['publish_date'] = df['publish_date'].replace(i, mth)\n",
    "df['author'] = df['author'].fillna(\"\")\n",
    "replace_simb = ['[', ']', \"'\", ' etc.', \"‘\"]\n",
    "coma = \", \"\n",
    "array_col = []\n",
    "for i in df['author']:\n",
    "    temp = []\n",
    "    for j in replace_simb:\n",
    "        if j in i:\n",
    "            i = i.replace(j, \"\")\n",
    "            \n",
    "df['political_bias'] = df['political_bias'].fillna(\"NEUTRAL\")\n",
    "df['image'] = df['image'].fillna(\"\")\n",
    "df['title'] = df['title'].fillna(\"\")\n",
    "df['country'] = df['country'].fillna(\"UNKNOWN\")\n",
    "\n",
    "df['alltext'] = df['publisher'] + \" \" + df['author'] + \" \" +df['political_bias'] + \" \" + df['title'] + \" \" + df['body_text'] \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a6e987",
   "metadata": {},
   "source": [
    "#### Preprocessing | Basic Steps\n",
    "\n",
    "    1. Lowercase. \n",
    "    2. Stop words.\n",
    "    3. Punctuation/Symbols.\n",
    "    4. Apostophre.\n",
    "    5. Single characters.\n",
    "    6. Stemming.\n",
    "    7. Lemmatisation.\n",
    "    8. Converting Numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c5ce545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toLowerCase(df):\n",
    "    df['alltext'] = df['alltext'].str.lower()\n",
    "    return df\n",
    "\n",
    "def delStopWords(df):\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    valid_allt=[]\n",
    "    '''all_text = []\n",
    "    for i in df['alltext']:\n",
    "        i = i.split()\n",
    "        all_text.append(i)\n",
    "    df['alltext']=all_text'''\n",
    "    for i in df['alltext']:\n",
    "        valid_t=[]\n",
    "        for j in i:\n",
    "            if j not in stop_words:\n",
    "                valid_t.append(j)\n",
    "        valid_allt.append(valid_t)\n",
    "    df['alltext']=valid_allt\n",
    "    return df\n",
    "\n",
    "def toListOfLists(df):\n",
    "    all_text = []\n",
    "    for i in df['alltext']:\n",
    "        i = i.split()\n",
    "        all_text.append(i)\n",
    "    df['alltext']=all_text\n",
    "    return df  \n",
    "\n",
    "def delSymbols(df):\n",
    "    symbols = \"!\\\"“‘’”#$%&()*+-./:;<,=>?@[\\]^_`{|}~\\n►●…\"\n",
    "    valid_allt=[]\n",
    "    for i in df['alltext']:\n",
    "        valid_t = []\n",
    "        for j in i:\n",
    "            for k in symbols:\n",
    "                if k in j:\n",
    "                    j = j.replace(k, \"\")\n",
    "            valid_t.append(j)\n",
    "        if \"[]\" in valid_t:\n",
    "            valid_t.remove(\"[]\")\n",
    "        valid_allt.append(valid_t)\n",
    "    df['alltext']=valid_allt\n",
    "    return df\n",
    "\n",
    "def delApostrophe(df):\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    valid_alltext=[]\n",
    "    for i in df['alltext']:\n",
    "        valid_text=[]\n",
    "        for j in i:\n",
    "            if j not in stop_words:\n",
    "                if \"'\" in j:\n",
    "                    j=j.replace(\"'\", \"\")\n",
    "                valid_text.append(j)\n",
    "        valid_alltext.append(valid_text)\n",
    "    df['alltext']=valid_alltext\n",
    "    return df\n",
    "\n",
    "def delSingleChars(df):\n",
    "    valid_allt=[]\n",
    "    for i in df['alltext']:\n",
    "        valid_t = []\n",
    "        for j in i:\n",
    "            if len(j)>1:\n",
    "                valid_t.append(j)\n",
    "        valid_allt.append(valid_t)\n",
    "    df['alltext']=valid_allt\n",
    "    return df\n",
    "\n",
    "#NOMBRES PROPIOS CON PUNTOS.\n",
    "#Get names w/ dots\n",
    "def remainDomains(df):\n",
    "    domain_dot=[]\n",
    "    for i in df['publisher']:\n",
    "        if \".\" in i:\n",
    "            domain_dot.append(str(i).lower())\n",
    "    domain_dot = np.unique(domain_dot)\n",
    "    #Rename those who aren't w/ dot but has to be with dot\n",
    "    valid_allt=[]\n",
    "    for i in df['alltext']:\n",
    "        valid_t=[]\n",
    "        for j in i:\n",
    "            for k in domain_dot:\n",
    "                if \".\" in k:\n",
    "                    k_wodot = k.replace(\".\", \"\")\n",
    "                    if k_wodot in j:\n",
    "                        j = j.replace(j, k)\n",
    "            valid_t.append(j)\n",
    "        valid_allt.append(valid_t)\n",
    "    df['alltext']=valid_allt    \n",
    "    return df\n",
    "\n",
    "#THE ORDER IS LEMMATIZATION THEN STEMMING, OR JUST STEMMING.\n",
    "def lemmatize(df):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    valid_allt=[]\n",
    "    for i in df['alltext']:\n",
    "        valid_t=[]\n",
    "        for j in i:\n",
    "            j = wnl.lemmatize(j)\n",
    "            valid_t.append(j)\n",
    "        valid_allt.append(valid_t) \n",
    "    df['alltext']=valid_allt\n",
    "    return df\n",
    "\n",
    "#STEMMING\n",
    "def stem(df):\n",
    "    stemmer = PorterStemmer()\n",
    "    valid_allt=[]\n",
    "    for i in df['alltext']:\n",
    "        valid_t=[]\n",
    "        for j in i:\n",
    "            j = stemmer.stem(j)\n",
    "            valid_t.append(j)\n",
    "        valid_allt.append(valid_t) \n",
    "    df['alltext']=valid_allt\n",
    "    return df\n",
    "    #df.to_csv('preprocessing-numbers.csv', index=False)\n",
    "    \n",
    "#converting numbers\n",
    "def num_conversion(j):\n",
    "    k, l = j.split()\n",
    "    k = float(k)\n",
    "    l = float(l)\n",
    "    j = str(k*l)\n",
    "    return j\n",
    "\n",
    "def convertNumbers(df):\n",
    "    valid_allt=[]\n",
    "    for i in df['alltext']:\n",
    "        valid_t = []\n",
    "        for j in i:\n",
    "            if j.isnumeric():\n",
    "                #print(j)\n",
    "                if \"½\" in j:\n",
    "                    j = j.replace(\"½\", \" 0.5\")\n",
    "                    j = num_conversion(j)\n",
    "                if \"¼\" in j:\n",
    "                    #print(j)\n",
    "                    j = j.replace(\"¼\", \" 0.25\")\n",
    "                    j = num_conversion(j)\n",
    "                if \"⅔\" in j:\n",
    "                    j = j.replace(\"⅔\", \" 0.67\")\n",
    "                    j = num_conversion(j)\n",
    "                if \"¾\" in j:\n",
    "                    j = j.replace(\"¾\", \" 0.75\")\n",
    "                    j = num_conversion(j)\n",
    "                if \"⅓\" in j:\n",
    "                    j = j.replace(\"⅓\", \" 0.33\")\n",
    "                    j = num_conversion(j)\n",
    "                j = num2words.num2words(float(j))\n",
    "            valid_t.append(j)          \n",
    "        valid_allt.append(valid_t)\n",
    "    df['alltext']=valid_allt\n",
    "    return df\n",
    "    #df.to_csv('preprocessing-no_numbers.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b895e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df = toLowerCase(df)\n",
    "    df = toListOfLists(df)\n",
    "    df = delStopWords(df)\n",
    "    df = delSymbols(df)\n",
    "    df = delStopWords(df)\n",
    "    df = delApostrophe(df)\n",
    "    df = delSingleChars(df)\n",
    "    df = lemmatize(df)\n",
    "    df = stem(df)\n",
    "    df = convertNumbers(df)\n",
    "    df = delSymbols(df)\n",
    "    df = remainDomains(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d11c166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(df)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210e18c3",
   "metadata": {},
   "source": [
    "## TF-IDF Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bb3528",
   "metadata": {},
   "source": [
    "#### TF (Term Frequency). TF is individual to each document and word. Frequency of a term in relation to the doc that belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8a7bb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF(t,d) = count of t in d / number of words in d\n",
    "#Array of dicts. Where each dict, Key: unique word in doc. Value: frequency of that word appearing in that document. \n",
    "tf_all = []\n",
    "n_docs = len(df['alltext'])\n",
    "#Dict. Key: news_id. Value: total number of words of that new.\n",
    "n_words_per_doc = {}\n",
    "w = 0\n",
    "for i in df['alltext']:\n",
    "    term_freq = {} #for each doc\n",
    "    uniq_vals = pd.unique(i) #uniq vals for each doc\n",
    "    n_words = len(i) # number of words for each doc\n",
    "    n_words_per_doc[w] = n_words #dict/set of doc - n_words per doc\n",
    "    w+=1\n",
    "    n_uniq_vals = len(uniq_vals)\n",
    "    for x in range(n_uniq_vals): #recorremos todos los valores unicos por doc\n",
    "        n=0\n",
    "        temp = uniq_vals[x]\n",
    "        for j in i:\n",
    "            if j==temp: \n",
    "                n+=1\n",
    "        term_freq[temp]=n #frecuencia de términos por doc\n",
    "    tf_all.append(term_freq) #array of dicts de term-termfreq\n",
    "tf_per_doc = []\n",
    "for x in range(n_docs): \n",
    "    tf = {}\n",
    "    dict_ = tf_all[x] #dict de frequencies de doc\n",
    "    nw = n_words_per_doc[x] #número de words de doc\n",
    "    for y in dict_.keys(): #recorrer keys: unique vals. por cada uniq val in doc\n",
    "        freq = dict_.get(y) #get freq\n",
    "        #Calcular tf. freq/#words\n",
    "        tf_ = freq/nw #normalizamos\n",
    "        tf[y]=tf_ #agregamos a dict\n",
    "    tf_per_doc.append(tf) #array de dicts de term-termfreq normalizado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c108ed",
   "metadata": {},
   "source": [
    "#### DF (Document Frequency). Number of documents in which the word is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86efa2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df(t) = occurrence of t in N documents.\n",
    "#Get all distinct words in all the docs.\n",
    "words_ = []\n",
    "n=0\n",
    "for x in range(n_docs): \n",
    "    dict_ = tf_all[x] #por cada dict de frequencies de cada doc\n",
    "    for y in dict_.keys(): #recorrer keys: unique vals.\n",
    "        words_.append(y)\n",
    "        n+=1\n",
    "words = np.unique(words_) #get todas las palabras distintas que aparecen en todos los docs\n",
    "nw = len(words) #numero de palabras distitnas en todos los docs\n",
    "n_news = len(tf_all)\n",
    "doc_freq={}\n",
    "for x in range(nw):\n",
    "    n=0\n",
    "    temp = words[x]\n",
    "    for y in range(n_news):\n",
    "        dict_ = tf_all[y]\n",
    "        vals = dict_.keys()\n",
    "        if temp in vals:\n",
    "            n+=1 #si palabra presente en cada dict de cada doc, sumamos más 1 (nos interesa presencia, no frecuencia)\n",
    "            continue\n",
    "    doc_freq[temp] = n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e5d9c0",
   "metadata": {},
   "source": [
    "#### IDF (Inverse Document Frequency). Informativeness of term t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af0b51a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#idf(t) = N/df. Df are the values for each key (unique words in all the docs). And N, the number of docs.\n",
    "#idf(t) = log(N/(df + 1)).\n",
    "idf={}\n",
    "n_docs = len(df['alltext'])\n",
    "for x in doc_freq.keys(): #recorremos dict de df que vamos a invertir\n",
    "    df_=doc_freq.get(x)\n",
    "    idf_=math.log(n_docs/(df_+1))\n",
    "    idf[x]=idf_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a256ff70",
   "metadata": {},
   "source": [
    "#### TF-IDF (Term Frequency - Inverse Document Frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26f8105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf(t, d) = tf(t, d) * log(N/(df + 1)).\n",
    "tf_idf_allt = []\n",
    "for x in range(len(tf_per_doc)):\n",
    "    tf_idf={}\n",
    "    dict_tf_doc = tf_per_doc[x]\n",
    "    for y in dict_tf_doc.keys():\n",
    "        tf_r = dict_tf_doc.get(y)\n",
    "        idf_r = idf.get(y)\n",
    "        tf_idf_ = tf_r*idf_r\n",
    "        tf_idf[y]=tf_idf_\n",
    "    tf_idf_allt.append(tf_idf) \n",
    "df['tf-idf']=tf_idf_allt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a57cc8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>image</th>\n",
       "      <th>body_text</th>\n",
       "      <th>political_bias</th>\n",
       "      <th>country</th>\n",
       "      <th>reliability</th>\n",
       "      <th>alltext</th>\n",
       "      <th>tf-idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>January</td>\n",
       "      <td>['Knvul Sheikh', 'Roni Caryn Rabin']</td>\n",
       "      <td>The Coronavirus: What Scientists Have Learned ...</td>\n",
       "      <td>https://static01.nyt.com/images/2020/03/12/sci...</td>\n",
       "      <td>\\nA novel respiratory virus that originated in...</td>\n",
       "      <td>Left</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>[new, york, time, knvul, sheikh, roni, caryn, ...</td>\n",
       "      <td>{'new': 0.005144562828399132, 'york': 0.001377...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>npr.org</td>\n",
       "      <td>National Public Radio (NPR)</td>\n",
       "      <td>January</td>\n",
       "      <td>['Emily Feng']</td>\n",
       "      <td>Chinese Health Officials: More Die From Newly ...</td>\n",
       "      <td>https://media.npr.org/include/images/facebook-...</td>\n",
       "      <td>Chinese Health Officials: More Die From Newly ...</td>\n",
       "      <td>Center</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>[nation, public, radio, npr, emili, feng, cent...</td>\n",
       "      <td>{'nation': 0.0034242938862532523, 'public': 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>theverge.com</td>\n",
       "      <td>The Verge</td>\n",
       "      <td>January</td>\n",
       "      <td>['Nicole Wetsman']</td>\n",
       "      <td>Everything you need to know about the coronavirus</td>\n",
       "      <td>https://cdn.vox-cdn.com/thumbor/a9_Oz7cvSBKyal...</td>\n",
       "      <td>Public health experts around the globe are scr...</td>\n",
       "      <td>Left-center</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>[verg, nicol, wetsman, leftcent, everyth, need...</td>\n",
       "      <td>{'verg': 0.0038149774052306145, 'nicol': 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>worldhealth.net</td>\n",
       "      <td>WorldHealth.Net</td>\n",
       "      <td>January</td>\n",
       "      <td>[]</td>\n",
       "      <td>Novel Coronavirus Cases Confirmed To Be Spreading</td>\n",
       "      <td>https://www.worldhealth.net/media/original_ima...</td>\n",
       "      <td>The first two coronavirus cases in Europe have...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>USA</td>\n",
       "      <td>0</td>\n",
       "      <td>[worldhealth.net, neutral, novel, coronaviru, ...</td>\n",
       "      <td>{'worldhealth.net': 0.004356187002041755, 'neu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>theverge.com</td>\n",
       "      <td>The Verge</td>\n",
       "      <td>January</td>\n",
       "      <td>['Nicole Wetsman', 'Zoe Schiffer', 'Jay Peters...</td>\n",
       "      <td>Coronavirus disrupts the world: updates on the...</td>\n",
       "      <td>https://cdn.vox-cdn.com/thumbor/t2gt1SmEni4Mcr...</td>\n",
       "      <td>A new coronavirus appeared in Wuhan, China, at...</td>\n",
       "      <td>Left-center</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>[verg, nicol, wetsman, zoe, schiffer, jay, pet...</td>\n",
       "      <td>{'verg': 0.01878670527971479, 'nicol': 0.03134...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   news_id              url                    publisher publish_date  \\\n",
       "0        0      nytimes.com           The New York Times      January   \n",
       "1        1          npr.org  National Public Radio (NPR)      January   \n",
       "2        2     theverge.com                    The Verge      January   \n",
       "3        3  worldhealth.net              WorldHealth.Net      January   \n",
       "4        4     theverge.com                    The Verge      January   \n",
       "\n",
       "                                              author  \\\n",
       "0               ['Knvul Sheikh', 'Roni Caryn Rabin']   \n",
       "1                                     ['Emily Feng']   \n",
       "2                                 ['Nicole Wetsman']   \n",
       "3                                                 []   \n",
       "4  ['Nicole Wetsman', 'Zoe Schiffer', 'Jay Peters...   \n",
       "\n",
       "                                               title  \\\n",
       "0  The Coronavirus: What Scientists Have Learned ...   \n",
       "1  Chinese Health Officials: More Die From Newly ...   \n",
       "2  Everything you need to know about the coronavirus   \n",
       "3  Novel Coronavirus Cases Confirmed To Be Spreading   \n",
       "4  Coronavirus disrupts the world: updates on the...   \n",
       "\n",
       "                                               image  \\\n",
       "0  https://static01.nyt.com/images/2020/03/12/sci...   \n",
       "1  https://media.npr.org/include/images/facebook-...   \n",
       "2  https://cdn.vox-cdn.com/thumbor/a9_Oz7cvSBKyal...   \n",
       "3  https://www.worldhealth.net/media/original_ima...   \n",
       "4  https://cdn.vox-cdn.com/thumbor/t2gt1SmEni4Mcr...   \n",
       "\n",
       "                                           body_text political_bias country  \\\n",
       "0  \\nA novel respiratory virus that originated in...           Left     USA   \n",
       "1  Chinese Health Officials: More Die From Newly ...         Center     USA   \n",
       "2  Public health experts around the globe are scr...    Left-center     USA   \n",
       "3  The first two coronavirus cases in Europe have...        NEUTRAL     USA   \n",
       "4  A new coronavirus appeared in Wuhan, China, at...    Left-center     USA   \n",
       "\n",
       "   reliability                                            alltext  \\\n",
       "0            1  [new, york, time, knvul, sheikh, roni, caryn, ...   \n",
       "1            1  [nation, public, radio, npr, emili, feng, cent...   \n",
       "2            1  [verg, nicol, wetsman, leftcent, everyth, need...   \n",
       "3            0  [worldhealth.net, neutral, novel, coronaviru, ...   \n",
       "4            1  [verg, nicol, wetsman, zoe, schiffer, jay, pet...   \n",
       "\n",
       "                                              tf-idf  \n",
       "0  {'new': 0.005144562828399132, 'york': 0.001377...  \n",
       "1  {'nation': 0.0034242938862532523, 'public': 0....  \n",
       "2  {'verg': 0.0038149774052306145, 'nicol': 0.003...  \n",
       "3  {'worldhealth.net': 0.004356187002041755, 'neu...  \n",
       "4  {'verg': 0.01878670527971479, 'nicol': 0.03134...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
